
import requests
from bs4 import BeautifulSoup
import pandas as pd
import os

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36"
}

def clean_text(text):
    """Remove unwanted Unicode characters that can corrupt Excel files"""
    if not text:
        return "N/A"
    # Remove zero-width space and other problematic Unicode characters
    cleaned = text.replace('\u200b', '').replace('\u200c', '').replace('\u200d', '')
    # Replace multiple spaces with single space
    cleaned = ' '.join(cleaned.split())
    return cleaned.strip() or "N/A"

def scrape_promotions_page(page_num=1):
    """Scrape promotions from a specific page"""
    if page_num == 1:
        url = "https://www.ababank.com/promotions/"
    else:
        # For pages beyond 1, we'll try to find the actual pagination links from the first page
        url = f"https://www.ababank.com/promotions/?tx_news_pi1%5B%40widget_0%5D%5BcurrentPage%5D={page_num}"

    print(f"Scraping page {page_num}: {url}")
    response = requests.get(url, headers=headers)

    if response.status_code != 200:
        print(f"Failed to access page {page_num}. Status code: {response.status_code}")
        return [], False

    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Check if we're being blocked by CloudFlare or similar
    if "Just a moment" in soup.title.text if soup.title else "":
        print(f"Page {page_num} is protected by anti-bot measures")
        return [], False
    
    promotions = soup.find_all('div', class_='article articletype-0')
    print(f"Found {len(promotions)} promotions on page {page_num}")

    # Check if there are more pages by looking for pagination
    has_more_pages = False
    pagination = soup.find('ul', class_='pagination') or soup.find('div', class_='pagination')
    if pagination and page_num == 1:  # Only check on first page to see max pages
        page_links = pagination.find_all('a')
        for link in page_links:
            if link.text.isdigit():
                max_page = max(int(link.text), 1)
                has_more_pages = max_page > page_num

    return promotions, has_more_pages

def scrape_all_promotions(max_pages=10):
    """Scrape promotions from multiple pages"""
    all_data = []
    promotion_number = 1
    
    for page_num in range(1, max_pages + 1):
        promotions, has_more_pages = scrape_promotions_page(page_num)
        
        if not promotions:
            if page_num == 1:
                print("No promotions found on first page. Stopping.")
                break
            else:
                print(f"No more promotions found on page {page_num}. Stopping.")
                break
        
        # Process promotions from this page
        for i, promo in enumerate(promotions):
            print(f"Processing promotion {promotion_number}")    

            title_raw = promo.find('h3').text.strip() if promo.find('h3') else None
            date_raw = promo.find('p').text.strip() if promo.find('p') else None
            description_raw = promo.find('p', class_='bodytext').text.strip() if promo.find('p', class_='bodytext') else None

            # Clean the text
            title = clean_text(title_raw)
            date = clean_text(date_raw) 
            description = clean_text(description_raw)

            # Try to find link
            link = None
            link_elem = promo.find('a')
            if link_elem and link_elem.get('href'):
                link = link_elem['href']
                if link.startswith('/'):
                    link = "https://www.ababank.com" + link

            print(f"  Title: {title}")
            print(f"  Date: {date}")
            print(f"  Description: {description[:50] + '...' if description and len(description) > 50 else description}")
            print(f"  Link: {link}")
            print()

            all_data.append({
                'No.': promotion_number,
                'Title': title,
                'Date': date,
                'Description': description,
                'Link': link or "N/A"
            })
            
            promotion_number += 1
        
        # If we only got promotions from page 1, or if we can't access more pages, stop
        if page_num > 1 and not promotions:
            break
            
        # Add a small delay between requests to be respectful
        import time
        time.sleep(1)
    
    return all_data

# Main execution
print("Starting ABA Bank promotions scraper...")
print("This will attempt to scrape multiple pages of promotions.")

# Scrape promotions from multiple pages (adjust max_pages as needed)
data = scrape_all_promotions(max_pages=5)

print(f"Total data entries: {len(data)}")

if data:
    df = pd.DataFrame(data)    
   
    df.to_excel('aba_promotions.xlsx', index=False)
    print("Data has been successfully scraped and saved to aba_promotions.xlsx.")
    
    # Show preview of data
    print("\nPreview of scraped data:")
    print(df.head())
    print(f"\nTotal promotions scraped: {len(data)}")
else:
    print("No data was scraped. The page structure might have changed or elements not found.")
    
    # Save the HTML for inspection
    response = requests.get("https://www.ababank.com/promotions/", headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    with open('page_source.html', 'w', encoding='utf-8') as f:
        f.write(soup.prettify())
    print("Page source saved to page_source.html for inspection.")
